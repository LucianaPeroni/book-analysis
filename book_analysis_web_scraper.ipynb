{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "book-analysis-web scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONBF3ZUJwF/9zH3rc2cyC5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucianaPeroni/book-analysis/blob/main/book_analysis_web_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncrr8QmyyoF7",
        "outputId": "e6b83bd0-08ae-4a16-cdab-1c8589b03a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (4.1.3)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.7/dist-packages (from selenium) (0.20.0)\n",
            "Requirement already satisfied: urllib3[secure,socks]~=1.26 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.26.9)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.7/dist-packages (from selenium) (0.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.1.0)\n",
            "Requirement already satisfied: async-generator>=1.9 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.10)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.7/dist-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
            "Requirement already satisfied: cryptography>=1.3.4 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (36.0.2)\n",
            "Requirement already satisfied: pyOpenSSL>=0.14 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (22.0.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install webdriver-manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAN_SowSzKEW",
        "outputId": "a3ca360e-8a2c-40f7-f152-c9a182e2e0ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.7/dist-packages (3.5.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from webdriver-manager) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (2021.10.8)\n",
            "Installing collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.9\n",
            "    Uninstalling urllib3-1.26.9:\n",
            "      Successfully uninstalled urllib3-1.26.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "selenium 4.1.3 requires urllib3[secure,socks]~=1.26, but you have urllib3 1.25.11 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selectorlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfwUQ-gGzTMc",
        "outputId": "a103fe3c-454c-4870-ed4b-770480e5d827"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selectorlib in /usr/local/lib/python3.7/dist-packages (0.16.0)\n",
            "Requirement already satisfied: pyyaml>=3.12 in /usr/local/lib/python3.7/dist-packages (from selectorlib) (3.13)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from selectorlib) (7.1.2)\n",
            "Requirement already satisfied: parsel>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from selectorlib) (1.6.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.1->selectorlib) (4.2.6)\n",
            "Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.1->selectorlib) (1.15.0)\n",
            "Requirement already satisfied: w3lib>=1.19.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.1->selectorlib) (1.22.0)\n",
            "Requirement already satisfied: cssselect>=0.9 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.1->selectorlib) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jJxPQPc3H40",
        "outputId": "e56c0684-cd2a-4bfd-dad1-07b6d940958a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Amazon Web Sraper - Data Science para neg√≥cios"
      ],
      "metadata": {
        "id": "nSbzVEK_2btm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requisitos:\n",
        "\n",
        "*   Selenium\n",
        "*   Webdriver\n",
        "*   Sectorlib\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f6hL8D1U2kqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selectorlib import Extractor\n",
        "import requests\n",
        "import json\n",
        "import time\n"
      ],
      "metadata": {
        "id": "VU9-fqUz0_yV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Startup the webdriver"
      ],
      "metadata": {
        "id": "mr6-UZn_3MKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://www.amazon.com.br/Data-Science-para-neg%C3%B3cios-Fawcett/dp/8576089726/ref=sr_1_5?__mk_pt_BR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=S187FLBJZ8GS&keywords=data+science&qid=1649702411&sprefix=data+scienc%2Caps%2C269&sr=8-5&ufe=app_do%3Aamzn1.fos.6d798eae-cadf-45de-946a-f477d47705b9\"\n",
        "page = requests.get(URL)\n",
        "\n",
        "print(page.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbZ1na4esm94",
        "outputId": "3b121895-958e-4430-ab49-5ea913141369"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!doctype html><html><head><meta charset=\"utf-8\"><meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\"><meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\"><title>Amazon.com.br Algo deu errado</title><style>html,body{padding:0;margin:0}img{border:0}#a,#b{background:#232f3e;padding:11px;height:35px}#c{position:absolute;left:22px;top:12px}#e{position:relative;max-width:800px;padding:0 40px 0 171px}#f,#g{height:35px;border:0;font-size:1em}#f{width:100%;margin:0;padding:0 10px;border-radius:4px 0 0 4px}#g{cursor:pointer;background:#febd69;font-weight:bold;border-radius:0 4px 4px 0;-webkit-appearance:none;position:absolute;top:0;right:0;padding:0 12px}@media(max-width:500px){#e{padding-left:0}#b{padding:55px 10px 10px}#c{left:6px}}#h{text-align:center;margin:30px 0}#h img{max-width:90%}#d{display:none}#d[src]{display:inline}</style></head><body><form id=\"b\" accept-charset=\"utf-8\" action=\"/s\" method=\"GET\" role=\"search\"><a href=\"/\"><img id=\"c\" src=\"https://images-na.ssl-images-amazon.com/images/G/32/error/logo._TTD_.png\" alt=\"Amazon.com.br\"></a><div id=\"e\"><input id=\"f\" name=\"field-keywords\" placeholder=\"Buscar\"><input id=\"g\" type=\"submit\" value=\"Ir\"></div></form><div id=\"h\"><div><a href=\"/\"><img src=\"https://images-na.ssl-images-amazon.com/images/G/32/error/500-title._TTD_.png\" alt=\"Desculpe! Algo deu errado. Tente novamente ou volte para a p√É¬°gina inicial da Amazon.\"></a></div><img id=\"d\" alt=\"Cachorros da Amazon\"><script>document.getElementById(\"d\").src=\"https://images-na.ssl-images-amazon.com/images/G/32/error/\"+(Math.floor(Math.random()*200)+1)+\"._TTD_.jpg\";</script></div></body></html>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_amazon(item):\n",
        "\n",
        "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
        "    driver.get('https://www.amazon.com.br/Data-Science-para-neg%C3%B3cios-Fawcett/dp/8576089726/ref=pd_rhf_dp_s_ci_mcx_mr_hp_d_sccl_1/133-9449843-1989261?pd_rd_w=FuzOW&pf_rd_p=1d197d3f-52b3-48f9-8e03-2918da01793e&pf_rd_r=ZDQ0X750ZXEM45DEEDBH&pd_rd_r=154c9646-49de-469b-933b-473b7d3e192c&pd_rd_wg=Ldy3U&pd_rd_i=8576089726&psc=1')\n",
        "    search_box = driver.find_element_by_id('twotabsearchtextbox').send_keys(item)\n",
        "    search_button = driver.find_element_by_id(\"nav-search-submit-text\").click()\n",
        "\n",
        "    driver.implicitly_wait(5)\n",
        "\n",
        "    try:\n",
        "        num_page = driver.find_element_by_xpath('//*[@id=\"dp-container\"]/div[1]')\n",
        "    except NoSuchElementException:\n",
        "        num_page = driver.find_element_by_class_name('a-last').click()\n",
        "\n",
        "    driver.implicitly_wait(3)\n",
        "\n",
        "    url_list = []\n",
        "\n",
        "    for i in range(int(num_page.text)):\n",
        "        page_ = i + 1\n",
        "        url_list.append(driver.current_url)\n",
        "        driver.implicitly_wait(4)\n",
        "        click_next = driver.find_element_by_class_name('a-last').click()\n",
        "        print(\"Page \" + str(page_) + \" grabbed\")\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "\n",
        "    with open('search_results_urls.txt', 'w') as filehandle:\n",
        "        for result_page in url_list:\n",
        "            filehandle.write('%s\\n' % result_page)\n",
        "\n",
        "print(\"---DONE---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk3sFUgs5Keq",
        "outputId": "3b33f025-47c0-476c-815b-6a31f02bbfb8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---DONE---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape(url):\n",
        "\n",
        "    headers = {\n",
        "        'dnt': '1',\n",
        "        'upgrade-insecure-requests': '1',\n",
        "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36',\n",
        "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "        'sec-fetch-site': 'same-origin',\n",
        "        'sec-fetch-mode': 'navigate',\n",
        "        'sec-fetch-user': '?1',\n",
        "        'sec-fetch-dest': 'document',\n",
        "        'referer': 'https://www.amazon.com/',\n",
        "        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
        "    }\n",
        "\n",
        "    # Download the page using requests\n",
        "    print(\"Downloading %s\"%url)\n",
        "    r = requests.get(url, headers=headers)\n",
        "    # Simple check to check if page was blocked (Usually 503)\n",
        "    if r.status_code > 500:\n",
        "        if \"To discuss automated access to Amazon data please contact\" in r.text:\n",
        "            print(\"Page %s was blocked by Amazon. Please try using better proxies\\n\"%url)\n",
        "        else:\n",
        "            print(\"Page %s must have been blocked by Amazon as the status code was %d\"%(url,r.status_code))\n",
        "        return None\n",
        "    # Pass the HTML of the page and create\n",
        "    return e.extract(r.text)\n",
        "\n",
        "\n",
        "search_amazon(search_query) # <------ search query goes here.\n",
        "\n",
        "# Create an Extractor by reading from the YAML file\n",
        "e = Extractor.from_yaml_file('search_results.yml')\n",
        "\n",
        "# product_data = []\n",
        "output_file = open('{}_{}_results.jsonl'.format(search_query,date), \"w+\")\n",
        "destination = 'results'\n",
        "\n",
        "with open(\"search_results_urls.txt\",'r') as urllist, open('{}_{}_results.jsonl'.format(search_query,date),'w') as outfile:\n",
        "    for url in urllist.read().splitlines():\n",
        "        data = scrape(url)\n",
        "        if data:\n",
        "            for product in data['products']:\n",
        "                product['search_url'] = url\n",
        "                print(\"Saving Product: %s\"%product['title'].encode('utf8'))\n",
        "                json.dump(product,outfile)\n",
        "                outfile.write(\"\\n\")\n",
        "                # sleep(5)\n",
        "\n",
        "new_path = shutil.move('{}_{}_results.jsonl'.format(search_query, date), destination)\n",
        "\n",
        "print(\"---DONE---\")\n",
        "print('\\n')\n",
        "¬© 2022 GitHub, Inc.\n",
        "Terms\n",
        "Privacy\n",
        "Security\n",
        "Status\n",
        "Docs\n",
        "Contact GitHub\n",
        "Pricing\n",
        "API\n",
        "Training\n",
        "Blog\n",
        "About\n"
      ],
      "metadata": {
        "id": "PPNZGryNL07A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}